{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch News Quotes and Labels\n",
    "The function below scans the Politfact webpage and extracts different information related to political quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FetchNews(label, page_start, page_end):\n",
    "    \"\"\"Function used to scrape the news articles page by page\n",
    "    \n",
    "    Args:\n",
    "        label (str): 'true', 'mostly-true', 'half-true', 'barely-true', 'false', 'pants-fire'\n",
    "        page_start (int): Number of first page\n",
    "        page_end (int): Number of last page\n",
    "        \n",
    "    Returns:\n",
    "        df_raw (DataFrame): dataframe containing the news quotes\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize empty dataframe\n",
    "    df_raw = pd.DataFrame()\n",
    "\n",
    "    # Iterate through range of pages (or until last available)\n",
    "    for page_num in range(page_start, page_end):\n",
    "        \n",
    "        # Fetch page = page_num, fetch all news articles\n",
    "        html = requests.get(f'https://www.politifact.com/factchecks/list/?page={page_num}&ruling={label}')\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        articles = soup.findAll('div', {'class': 'm-statement__quote'})\n",
    "\n",
    "        # If 'pfhead' class is found, it means the page couldn't be found; otherwise returns None\n",
    "        error = soup.find('div', {'class': 'pfhead'})\n",
    "\n",
    "        if error == None:\n",
    "            \n",
    "            print(f'Fetching {label} news page {page_num}...')\n",
    "\n",
    "            # Iterate through articles\n",
    "            for article in articles:\n",
    "                \n",
    "                # Fetch artcile page\n",
    "                url = re.search(r'<a href=\"(.*)\">', str(article)).group(1)\n",
    "                html = requests.get(f'https://www.politifact.com{url}')\n",
    "                soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "                # Fetch raw content from divs\n",
    "                quote_raw = soup.find('div', {'class': 'm-statement__quote'}).text\n",
    "                author_raw = soup.find('a', {'class': 'm-statement__name'}).text\n",
    "                context_raw = str(soup.find('div', {'class': 'm-statement__desc'}))\n",
    "                categories_raw = soup.findAll('li', {'class': 'm-list__item'})\n",
    "                staff_raw = str(soup.findAll('div', {'class': 'm-author__content'}))\n",
    "\n",
    "                # Clean up data a little\n",
    "                quote = quote_raw.strip()\n",
    "                author = author_raw.strip()\n",
    "                date_regex = re.search(r'on ([A-Za-z]+ \\d{1,2}, \\d{4}) in', context_raw)\n",
    "                date = date_regex.group(1) if date_regex is not None else 'unspecified'\n",
    "                context_regex = re.search(r'\\d{4} in?(.*)', context_raw)\n",
    "                context = context_regex.group(1).strip().strip(':') if context_regex is not None else 'unspecified'\n",
    "                categories = ', '.join(re.findall(r'title=\\\"(.*)\\\">', str(categories_raw[:-1])))\n",
    "                staff = ', '.join(re.findall(r'>(.*)</a>', staff_raw))\n",
    "    \n",
    "                # Create row\n",
    "                row = pd.DataFrame({\n",
    "                    'label': [label], \n",
    "                    'quote': [quote], \n",
    "                    'context': [context], \n",
    "                    'author': [author], \n",
    "                    'date': [date], \n",
    "                    'categories': [categories],\n",
    "                    'staff': [staff]\n",
    "                })\n",
    "\n",
    "                # Append row to dataframe\n",
    "                df_raw = df_raw.append(row)\n",
    "\n",
    "                # Sleep for a few seconds, be nice to web servers :)\n",
    "                pause = random.randint(3, 5)\n",
    "                #print(f'Fetched news from page {page_num}, sleeping for {pause} seconds.')\n",
    "                time.sleep(pause)\n",
    "\n",
    "        else:\n",
    "            page_end = page_num\n",
    "            break\n",
    "    \n",
    "    \n",
    "    print(f'Done! Fetched all {label} news from pages {page_start} to {page_end}.')\n",
    "    return(df_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it takes some time to retrieve the data, it's best to separate by label and do the extraction in segments. The `page_start` and `page_end` parameters allow us to set the range of pages to collect. Since the maximum number of pages changes as more news items are added with time, simply set `page_end` to a high value (ie. `1000`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching true news page 1...\n",
      "Fetching true news page 2...\n",
      "Done! Fetched all true news from page 1 to 3.\n"
     ]
    }
   ],
   "source": [
    "true_df = FetchNews('true', 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostly True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching mostly-true news page 1...\n",
      "Fetching mostly-true news page 2...\n",
      "Done! Fetched all mostly-true news from page 1 to 3.\n"
     ]
    }
   ],
   "source": [
    "mostly_true_df = FetchNews('mostly-true', 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching half-true news page 1...\n",
      "Fetching half-true news page 2...\n",
      "Done! Fetched all half-true news from page 1 to 3.\n"
     ]
    }
   ],
   "source": [
    "half_true_df = FetchNews('half-true', 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barely True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching barely-true news page 1...\n",
      "Fetching barely-true news page 2...\n",
      "Done! Fetched all barely-true news from page 1 to 3.\n"
     ]
    }
   ],
   "source": [
    "barely_true_df = FetchNews('barely-true', 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching false news page 1...\n",
      "Fetching false news page 2...\n",
      "Done! Fetched all false news from page 1 to 3.\n"
     ]
    }
   ],
   "source": [
    "false_df = FetchNews('false', 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pants on Fire News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching pants-fire news page 1...\n",
      "Fetching pants-fire news page 2...\n",
      "Done! Fetched all pants-fire news from page 1 to 3.\n"
     ]
    }
   ],
   "source": [
    "pants_fire_df = FetchNews('pants-fire', 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportDataFrame(df, filename):\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(filename + '.csv', index=False)\n",
    "\n",
    "# Export dataframes\n",
    "exportDataFrame(true_df, 'true.csv')\n",
    "exportDataFrame(mostly_true_df, 'mostly-true.csv')\n",
    "exportDataFrame(half_true_df, 'half-true.csv')\n",
    "exportDataFrame(barely_true_df, 'barely-true.csv')\n",
    "exportDataFrame(false_df, 'false.csv')\n",
    "exportDataFrame(pants_fire_df, 'pants-fire.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
