{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper\n",
    "In order to fetch the most up-to-date news quotes along with the labels, we will scrape the information from Politifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch News Quotes and Labels\n",
    "The function below scans the Politfact webpage and extracts different information related to political quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FetchNews(label, page_start, page_end, df=None):\n",
    "    \"\"\"Function used to scrape the news articles page by page\n",
    "    \n",
    "    Args:\n",
    "        label (str): 'true', 'mostly-true', 'half-true', 'barely-true', 'false', 'pants-fire'\n",
    "        page_start (int): Number of first page\n",
    "        page_end (int): Number of last page\n",
    "        \n",
    "    Returns:\n",
    "        df_raw (DataFrame): dataframe containing the news quotes\n",
    "    \n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        existing_quotes = []\n",
    "    else:\n",
    "        existing_quotes = list(df['quote'])\n",
    "    \n",
    "    # Initialize empty dataframe and variables\n",
    "    df_raw = pd.DataFrame()\n",
    "    export_vars = ['label', 'quote', 'context', 'author', 'date', 'categories', 'staff']\n",
    "\n",
    "    # Iterate through range of pages (or until last available)\n",
    "    for page_num in range(page_start, page_end):\n",
    "        \n",
    "        # Fetch page = page_num, fetch all news articles\n",
    "        html = requests.get(f'https://www.politifact.com/factchecks/list/?page={page_num}&ruling={label}')\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        articles = soup.findAll('div', {'class': 'm-statement__quote'})\n",
    "\n",
    "        # If 'pfhead' class is found, it means the page couldn't be found; otherwise returns None\n",
    "        error = soup.find('div', {'class': 'pfhead'})\n",
    "\n",
    "        if error == None:\n",
    "            \n",
    "            print(f'Fetching {label} news page {page_num}...')\n",
    "\n",
    "            # Iterate through articles\n",
    "            for article in articles:\n",
    "                \n",
    "                # Fetch artcile page\n",
    "                url = re.search(r'<a href=\"(.*)\">', str(article)).group(1)\n",
    "                html = requests.get(f'https://www.politifact.com{url}')\n",
    "                soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "                # Fetch raw content from divs\n",
    "                quote_raw = soup.find('div', {'class': 'm-statement__quote'}).text\n",
    "                author_raw = soup.find('a', {'class': 'm-statement__name'}).text\n",
    "                context_raw = str(soup.find('div', {'class': 'm-statement__desc'}))\n",
    "                categories_raw = soup.findAll('li', {'class': 'm-list__item'})\n",
    "                staff_raw = str(soup.findAll('div', {'class': 'm-author__content'}))\n",
    "\n",
    "                # Clean up data a little\n",
    "                quote = quote_raw.strip()\n",
    "                author = author_raw.strip()\n",
    "                date_regex = re.search(r'on ([A-Za-z]+ \\d{1,2}, \\d{4}) in', context_raw)\n",
    "                date = date_regex.group(1) if date_regex is not None else 'unspecified'\n",
    "                context_regex = re.search(r'\\d{4} in?(.*)', context_raw)\n",
    "                context = context_regex.group(1).strip().strip(':') if context_regex is not None else 'unspecified'\n",
    "                categories = ', '.join(re.findall(r'title=\\\"(.*)\\\">', str(categories_raw[:-1])))\n",
    "                staff = ', '.join(re.findall(r'>(.*)</a>', staff_raw))\n",
    "                \n",
    "                if quote not in existing_quotes:\n",
    "    \n",
    "                    # Create row\n",
    "                    row = pd.DataFrame({\n",
    "                        'label': [label], \n",
    "                        'quote': [quote], \n",
    "                        'context': [context], \n",
    "                        'author': [author], \n",
    "                        'date': [date], \n",
    "                        'categories': [categories],\n",
    "                        'staff': [staff]\n",
    "                    })\n",
    "\n",
    "                    # Append row to dataframe\n",
    "                    df_raw = df_raw.append(row, ignore_index=True)\n",
    "\n",
    "                    # Sleep for a few seconds, be nice to web servers :)\n",
    "                    pause = random.randint(3, 5)\n",
    "                    #print(f'Fetched news from page {page_num}, sleeping for {pause} seconds.')\n",
    "                    time.sleep(pause)\n",
    "                else:\n",
    "                    print(f'Entry already exists! Stopping execution...')\n",
    "                    print(f'Done! Updated dataset with {label} news from pages {page_start} to {page_num}.')\n",
    "                    return(df_raw.append(df, ignore_index=True).loc[:, export_vars])\n",
    "\n",
    "        else:\n",
    "            page_end = page_num\n",
    "            break\n",
    "    \n",
    "    \n",
    "    print(f'Done! Fetched all {label} news from pages {page_start} to {page_end}.')\n",
    "    return(df_raw.loc[:, export_vars])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it takes some time to retrieve the data, it's best to separate by label and do the extraction in segments. The `page_start` and `page_end` parameters allow us to set the range of pages to collect. Since the maximum number of pages changes as more news items are added with time, simply set `page_end` to a high value (ie. `1000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "metadata_path = 'metadata/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching true news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with true news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "# true_df = FetchNews('true', 1, 3)\n",
    "true_df = FetchNews('true', 1, 1000, pd.read_csv(data_path + 'true.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>quote</th>\n",
       "      <th>context</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>categories</th>\n",
       "      <th>staff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>true</td>\n",
       "      <td>When Donald Trump lost the Iowa caucus to Ted ...</td>\n",
       "      <td>tweets</td>\n",
       "      <td>Tweets</td>\n",
       "      <td>November 18, 2020</td>\n",
       "      <td>Elections, Iowa</td>\n",
       "      <td>Eleanor Hildebrandt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>true</td>\n",
       "      <td>\"We heard from the Department of Homeland Secu...</td>\n",
       "      <td>a TV interview</td>\n",
       "      <td>Tammy Baldwin</td>\n",
       "      <td>November 15, 2020</td>\n",
       "      <td>Criminal Justice, Elections, States, Wisconsin</td>\n",
       "      <td>Madeline Heim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>true</td>\n",
       "      <td>\"I’ve released 22 years of my tax returns. You...</td>\n",
       "      <td>a rally</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>October 31, 2020</td>\n",
       "      <td>National, Candidate Biography, Ethics, Taxes</td>\n",
       "      <td>Bill McCarthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>true</td>\n",
       "      <td>Farm bankruptcies are \"at an eight-year high.\"</td>\n",
       "      <td>comments at a campaign rally</td>\n",
       "      <td>Theresa Greenfield</td>\n",
       "      <td>October 30, 2020</td>\n",
       "      <td>Agriculture, Iowa</td>\n",
       "      <td>Lyle Muller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>true</td>\n",
       "      <td>Says Dan Forest has “missed almost half of the...</td>\n",
       "      <td>a debate</td>\n",
       "      <td>Roy Cooper</td>\n",
       "      <td>October 14, 2020</td>\n",
       "      <td>Education, North Carolina, Coronavirus</td>\n",
       "      <td>Paul Specht</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                              quote  \\\n",
       "0  true  When Donald Trump lost the Iowa caucus to Ted ...   \n",
       "1  true  \"We heard from the Department of Homeland Secu...   \n",
       "2  true  \"I’ve released 22 years of my tax returns. You...   \n",
       "3  true     Farm bankruptcies are \"at an eight-year high.\"   \n",
       "4  true  Says Dan Forest has “missed almost half of the...   \n",
       "\n",
       "                        context              author               date  \\\n",
       "0                        tweets              Tweets  November 18, 2020   \n",
       "1                a TV interview       Tammy Baldwin  November 15, 2020   \n",
       "2                       a rally           Joe Biden   October 31, 2020   \n",
       "3  comments at a campaign rally  Theresa Greenfield   October 30, 2020   \n",
       "4                      a debate          Roy Cooper   October 14, 2020   \n",
       "\n",
       "                                       categories                staff  \n",
       "0                                 Elections, Iowa  Eleanor Hildebrandt  \n",
       "1  Criminal Justice, Elections, States, Wisconsin        Madeline Heim  \n",
       "2    National, Candidate Biography, Ethics, Taxes        Bill McCarthy  \n",
       "3                               Agriculture, Iowa          Lyle Muller  \n",
       "4          Education, North Carolina, Coronavirus          Paul Specht  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostly True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching mostly-true news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with mostly-true news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "#mostly_true_df = FetchNews('mostly-true', 1, 3)\n",
    "mostly_true_df = FetchNews('mostly-true', 1, 1000, pd.read_csv(data_path + 'mostly-true.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching half-true news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with half-true news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "#half_true_df = FetchNews('half-true', 1, 3)\n",
    "half_true_df = FetchNews('half-true', 1, 1000, pd.read_csv(data_path + 'half-true.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barely True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching barely-true news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with barely-true news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "#barely_true_df = FetchNews('barely-true', 1, 3)\n",
    "barely_true_df = FetchNews('barely-true', 1, 1000, pd.read_csv(data_path + 'barely-true.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching false news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with false news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "#false_df = FetchNews('false', 1, 3)\n",
    "false_df = FetchNews('false', 1, 1000, pd.read_csv(data_path + 'false.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pants on Fire News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching pants-fire news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with pants-fire news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "#pants_fire_df = FetchNews('pants-fire', 1, 3)\n",
    "pants_fire_df = FetchNews('pants-fire', 1, 1000, pd.read_csv(data_path + 'pants-fire.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Personalities\n",
    "The function below scans the Politfact Personalities webpage and extracts different information related to each personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FetchPersonalities(index_start=None, index_end=None, df=None):\n",
    "    \"\"\"Function used to scrape the personalities\n",
    "    \n",
    "    Args:\n",
    "        None.\n",
    "        \n",
    "    Returns:\n",
    "        df_person_raw (DataFrame): dataframe containing the personalities\n",
    "    \n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        existing_personalities = []\n",
    "        df_raw = pd.DataFrame()\n",
    "    else:\n",
    "        existing_personalities = list(df['personality'])\n",
    "        df_raw = df\n",
    "    \n",
    "    # Initalize list for sorting later\n",
    "    sorter = []\n",
    "    \n",
    "    # Fetch page = page_num, fetch all personalities\n",
    "    html = requests.get(f'https://www.politifact.com/personalities/')\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    subjects = soup.findAll('div', {'class': 'c-chyron'})\n",
    "    \n",
    "    if index_start is None:\n",
    "        index_start = 0\n",
    "    \n",
    "    if index_end is None:\n",
    "        index_end = len(subjects)\n",
    "        \n",
    "    # Iterate through personalities\n",
    "    for subject in subjects[index_start:index_end]:\n",
    "        \n",
    "        # Fetch and clean personality and affiliation\n",
    "        personality_raw = subject.find('div', {'class': 'c-chyron__value'}).text\n",
    "        affiliation_raw = subject.find('div', {'class': 'c-chyron__subline'}).text\n",
    "        personality = re.sub(' +', ' ', personality_raw.strip())\n",
    "        affiliation = re.sub(' +', ' ', affiliation_raw.strip())\n",
    "        sorter.append(personality)\n",
    "        \n",
    "        if personality not in existing_personalities:\n",
    "            \n",
    "            print(f'Adding {personality}')\n",
    "            \n",
    "            # Fetch personality page\n",
    "            url = re.search(r'<a href=\"(.*)\">', str(subject)).group(1)\n",
    "            html = requests.get(f'https://www.politifact.com{url}')\n",
    "            soup = BeautifulSoup(html.text, 'html.parser')\n",
    "            \n",
    "            error = soup.find('div', {'class': 'pfhead'})\n",
    "            \n",
    "            if error is None:\n",
    "                # Fetch and clean description and link\n",
    "                description_raw = soup.find('div', {'class': 'm-pageheader__body'}).text\n",
    "                link_raw = soup.find('footer', {'class': 'm-pageheader__footer'})\n",
    "                description = description_raw.strip()\n",
    "                link = re.search(r' href=\"(.*?)\"', str(link_raw)).group(1)\n",
    "            else:\n",
    "                description = \"\"\n",
    "                link = \"\"\n",
    "\n",
    "            # Create row\n",
    "            row = pd.DataFrame({\n",
    "                'personality': [personality], \n",
    "                'affiliation': [affiliation], \n",
    "                'description': [description], \n",
    "                'link': [link],\n",
    "            })\n",
    "\n",
    "            # Append row to dataframe\n",
    "            df_raw = df_raw.append(row, ignore_index=True)\n",
    "\n",
    "            # Sleep for a few seconds, be nice to web servers :)\n",
    "            pause = random.randint(2, 4)\n",
    "            time.sleep(pause)\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Sort data the way it is presented originally\n",
    "    sorterIndex = dict(zip(sorter, range(len(sorter))))\n",
    "    df_raw['personality_rank'] = df_raw['personality'].map(sorterIndex)\n",
    "    df_raw.sort_values(by='personality_rank', inplace=True)\n",
    "    df_raw.drop('personality_rank', 1, inplace=True)\n",
    "    \n",
    "    print('Done fetching personalities!')\n",
    "    return(df_raw.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Kate Obenshain\n",
      "Done fetching personalities!\n"
     ]
    }
   ],
   "source": [
    "# If running for first time, simply remove argument df\n",
    "personalities_df = FetchPersonalities(df=pd.read_csv(metadata_path + 'personalities.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportDataFrame(df, filename):\n",
    "    \"\"\"Helper function to export dataframes\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): dataframe to export\n",
    "        filename (str): name of file to export\n",
    "    Returns:\n",
    "        None.\n",
    "        \n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export quotes dataframes\n",
    "exportDataFrame(true_df, data_path + 'true.csv')\n",
    "exportDataFrame(mostly_true_df, data_path + 'mostly-true.csv')\n",
    "exportDataFrame(half_true_df, data_path + 'half-true.csv')\n",
    "exportDataFrame(barely_true_df, data_path + 'barely-true.csv')\n",
    "exportDataFrame(false_df, data_path + 'false.csv')\n",
    "exportDataFrame(pants_fire_df, data_path + 'pants-fire.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export personalities dataframes\n",
    "exportDataFrame(personalities_df, data_path + 'personalities.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
