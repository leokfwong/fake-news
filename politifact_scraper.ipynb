{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper\n",
    "In order to fetch the most up-to-date news quotes along with the labels, we will scrape the information from Politifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch News Quotes and Labels\n",
    "The function below scans the Politfact webpage and extracts different information related to political quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FetchNews(label, page_start, page_end, df=None):\n",
    "    \"\"\"Function used to scrape the news articles page by page\n",
    "    \n",
    "    Args:\n",
    "        label (str): 'true', 'mostly-true', 'half-true', 'barely-true', 'false', 'pants-fire'\n",
    "        page_start (int): Number of first page\n",
    "        page_end (int): Number of last page\n",
    "        \n",
    "    Returns:\n",
    "        df_raw (DataFrame): dataframe containing the news quotes\n",
    "    \n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        existing_quotes = []\n",
    "    else:\n",
    "        existing_quotes = list(df['quote'])\n",
    "    \n",
    "    # Initialize empty dataframe and variables\n",
    "    df_raw = pd.DataFrame()\n",
    "    export_vars = ['label', 'quote', 'context', 'author_id', 'author_name', 'date', 'categories', 'staff']\n",
    "\n",
    "    # Iterate through range of pages (or until last available)\n",
    "    for page_num in range(page_start, page_end):\n",
    "        \n",
    "        # Fetch page = page_num, fetch all news articles\n",
    "        html = requests.get(f'https://www.politifact.com/factchecks/list/?page={page_num}&ruling={label}')\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        articles = soup.findAll('div', {'class': 'm-statement__quote'})\n",
    "\n",
    "        # If 'pfhead' class is found, it means the page couldn't be found; otherwise returns None\n",
    "        error = soup.find('div', {'class': 'pfhead'})\n",
    "\n",
    "        if error == None:\n",
    "            \n",
    "            print(f'Fetching {label} news page {page_num}...')\n",
    "\n",
    "            # Iterate through articles\n",
    "            for article in articles:\n",
    "                \n",
    "                # Fetch artcile page\n",
    "                url = re.search(r'<a href=\"(.*)\">', str(article)).group(1)\n",
    "                html = requests.get(f'https://www.politifact.com{url}')\n",
    "                soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "                # Fetch raw content from divs\n",
    "                quote_raw = soup.find('div', {'class': 'm-statement__quote'}).text\n",
    "                author_id_raw = soup.find('div', {'class': 'm-statement__meta'}).find('a', href=True)['href']\n",
    "                author_name_raw = soup.find('a', {'class': 'm-statement__name'}).text\n",
    "                context_raw = str(soup.find('div', {'class': 'm-statement__desc'}))\n",
    "                categories_raw = soup.findAll('li', {'class': 'm-list__item'})\n",
    "                staff_raw = str(soup.findAll('div', {'class': 'm-author__content'}))\n",
    "\n",
    "                # Clean up data a little\n",
    "                quote = quote_raw.strip()\n",
    "                author_id = re.search(r'/personalities/(.*)/', author_id_raw).group(1).strip()\n",
    "                author_name = author_name_raw.strip()\n",
    "                date_regex = re.search(r'on ([A-Za-z]+ \\d{1,2}, \\d{4})( in|:)', context_raw)\n",
    "                date = date_regex.group(1) if date_regex is not None else 'unspecified'\n",
    "                context_regex = re.search(r'\\d{4} in?(.*)', context_raw)\n",
    "                context = context_regex.group(1).strip().strip(':') if context_regex is not None else 'unspecified'\n",
    "                categories = ', '.join(re.findall(r'title=\\\"(.*)\\\">', str(categories_raw[:-1])))\n",
    "                staff = ', '.join(re.findall(r'>(.*)</a>', staff_raw))\n",
    "                \n",
    "                if quote not in existing_quotes:\n",
    "    \n",
    "                    # Create row\n",
    "                    row = pd.DataFrame({\n",
    "                        'label': [label], \n",
    "                        'quote': [quote], \n",
    "                        'context': [context],\n",
    "                        'author_id': [author_id],\n",
    "                        'author_name': [author_name], \n",
    "                        'date': [date], \n",
    "                        'categories': [categories],\n",
    "                        'staff': [staff]\n",
    "                    })\n",
    "\n",
    "                    # Append row to dataframe\n",
    "                    df_raw = df_raw.append(row, ignore_index=True)\n",
    "\n",
    "                    # Sleep for a few seconds, be nice to web servers :)\n",
    "                    pause = random.randint(2, 4)\n",
    "                    #print(f'Fetched news from page {page_num}, sleeping for {pause} seconds.')\n",
    "                    time.sleep(pause)\n",
    "                else:\n",
    "                    print(f'Entry already exists! Stopping execution...')\n",
    "                    print(f'Done! Updated dataset with {label} news from pages {page_start} to {page_num}.')\n",
    "                    return(df_raw.append(df, ignore_index=True).loc[:, export_vars])\n",
    "\n",
    "        else:\n",
    "            page_end = page_num\n",
    "            break\n",
    "    \n",
    "    \n",
    "    print(f'Done! Fetched all {label} news from pages {page_start} to {page_end}.')\n",
    "    return(df_raw.loc[:, export_vars])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it takes some time to retrieve the data, it's best to separate by label and do the extraction in segments. The `page_start` and `page_end` parameters allow us to set the range of pages to collect. Since the maximum number of pages changes as more news items are added with time, simply set `page_end` to a high value (ie. `1000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "metadata_path = 'metadata/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching true news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with true news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "#true_df = FetchNews('true', 1, 1000)\n",
    "true_df = FetchNews('true', 1, 1000, pd.read_csv(data_path + 'true.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>quote</th>\n",
       "      <th>context</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>categories</th>\n",
       "      <th>staff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>true</td>\n",
       "      <td>When Donald Trump lost the Iowa caucus to Ted ...</td>\n",
       "      <td>tweets</td>\n",
       "      <td>tweets</td>\n",
       "      <td>Tweets</td>\n",
       "      <td>November 18, 2020</td>\n",
       "      <td>Elections, Iowa</td>\n",
       "      <td>Eleanor Hildebrandt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>true</td>\n",
       "      <td>\"We heard from the Department of Homeland Secu...</td>\n",
       "      <td>a TV interview</td>\n",
       "      <td>tammy-baldwin</td>\n",
       "      <td>Tammy Baldwin</td>\n",
       "      <td>November 15, 2020</td>\n",
       "      <td>Criminal Justice, Elections, States, Wisconsin</td>\n",
       "      <td>Madeline Heim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>true</td>\n",
       "      <td>\"Iâ€™ve released 22 years of my tax returns. You...</td>\n",
       "      <td>a rally</td>\n",
       "      <td>joe-biden</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>October 31, 2020</td>\n",
       "      <td>National, Candidate Biography, Ethics, Taxes</td>\n",
       "      <td>Bill McCarthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>true</td>\n",
       "      <td>Farm bankruptcies are \"at an eight-year high.\"</td>\n",
       "      <td>comments at a campaign rally</td>\n",
       "      <td>theresa-greenfield</td>\n",
       "      <td>Theresa Greenfield</td>\n",
       "      <td>October 30, 2020</td>\n",
       "      <td>Agriculture, Iowa</td>\n",
       "      <td>Lyle Muller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>true</td>\n",
       "      <td>Says Dan Forest has â€œmissed almost half of the...</td>\n",
       "      <td>a debate</td>\n",
       "      <td>roy-cooper</td>\n",
       "      <td>Roy Cooper</td>\n",
       "      <td>October 14, 2020</td>\n",
       "      <td>Education, North Carolina, Coronavirus</td>\n",
       "      <td>Paul Specht</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                              quote  \\\n",
       "0  true  When Donald Trump lost the Iowa caucus to Ted ...   \n",
       "1  true  \"We heard from the Department of Homeland Secu...   \n",
       "2  true  \"Iâ€™ve released 22 years of my tax returns. You...   \n",
       "3  true     Farm bankruptcies are \"at an eight-year high.\"   \n",
       "4  true  Says Dan Forest has â€œmissed almost half of the...   \n",
       "\n",
       "                        context           author_id         author_name  \\\n",
       "0                        tweets              tweets              Tweets   \n",
       "1                a TV interview       tammy-baldwin       Tammy Baldwin   \n",
       "2                       a rally           joe-biden           Joe Biden   \n",
       "3  comments at a campaign rally  theresa-greenfield  Theresa Greenfield   \n",
       "4                      a debate          roy-cooper          Roy Cooper   \n",
       "\n",
       "                date                                      categories  \\\n",
       "0  November 18, 2020                                 Elections, Iowa   \n",
       "1  November 15, 2020  Criminal Justice, Elections, States, Wisconsin   \n",
       "2   October 31, 2020    National, Candidate Biography, Ethics, Taxes   \n",
       "3   October 30, 2020                               Agriculture, Iowa   \n",
       "4   October 14, 2020          Education, North Carolina, Coronavirus   \n",
       "\n",
       "                 staff  \n",
       "0  Eleanor Hildebrandt  \n",
       "1        Madeline Heim  \n",
       "2        Bill McCarthy  \n",
       "3          Lyle Muller  \n",
       "4          Paul Specht  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostly True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching mostly-true news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with mostly-true news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "#mostly_true_df = FetchNews('mostly-true', 1, 1000)\n",
    "mostly_true_df = FetchNews('mostly-true', 1, 1000, pd.read_csv(data_path + 'mostly-true.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching half-true news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with half-true news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "#half_true_df = FetchNews('half-true', 1, 1000)\n",
    "half_true_df = FetchNews('half-true', 1, 1000, pd.read_csv(data_path + 'half-true.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barely True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching barely-true news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with barely-true news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "#barely_true_df = FetchNews('barely-true', 1, 1000)\n",
    "barely_true_df = FetchNews('barely-true', 1, 1000, pd.read_csv(data_path + 'barely-true.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching false news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with false news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "#false_df = FetchNews('false', 1, 1000)\n",
    "false_df = FetchNews('false', 1, 1000, pd.read_csv(data_path + 'false.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pants on Fire News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching pants-fire news page 1...\n",
      "Entry already exists! Stopping execution...\n",
      "Done! Updated dataset with pants-fire news from pages 1 to 1.\n"
     ]
    }
   ],
   "source": [
    "#pants_fire_df = FetchNews('pants-fire', 1, 1000)\n",
    "pants_fire_df = FetchNews('pants-fire', 1, 1000, pd.read_csv(data_path + 'pants-fire.csv', dtype={'label':str}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Personalities\n",
    "The function below scans the Politfact Personalities webpage and extracts different information related to each personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FetchPersonalities(index_start=None, index_end=None, df=None):\n",
    "    \"\"\"Function used to scrape the personalities\n",
    "    \n",
    "    Args:\n",
    "        None.\n",
    "        \n",
    "    Returns:\n",
    "        df_person_raw (DataFrame): dataframe containing the personalities\n",
    "    \n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        existing_personalities = []\n",
    "        df_raw = pd.DataFrame()\n",
    "    else:\n",
    "        existing_personalities = list(df['author_id'])\n",
    "        df_raw = df\n",
    "    \n",
    "    # Initalize list for sorting later\n",
    "    sorter = []\n",
    "    \n",
    "    # Fetch page = page_num, fetch all personalities\n",
    "    html = requests.get(f'https://www.politifact.com/personalities/')\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    subjects = soup.findAll('div', {'class': 'c-chyron'})\n",
    "    \n",
    "    if index_start is None:\n",
    "        index_start = 0\n",
    "    \n",
    "    if index_end is None:\n",
    "        index_end = len(subjects)\n",
    "        \n",
    "    # Iterate through personalities\n",
    "    for subject in subjects[index_start:index_end]:\n",
    "        \n",
    "        # Fetch and clean personality and affiliation\n",
    "        author_id_raw = subject.find('div', {'class': 'c-chyron__value'}).find('a', href=True)['href']\n",
    "        author_name_raw = subject.find('div', {'class': 'c-chyron__value'}).text\n",
    "        affiliation_raw = subject.find('div', {'class': 'c-chyron__subline'}).text\n",
    "        \n",
    "        author_id = re.search(r'/personalities/(.*)/', author_id_raw).group(1).strip()\n",
    "        author_name = re.sub(' +', ' ', author_name_raw.strip())\n",
    "        affiliation = re.sub(' +', ' ', affiliation_raw.strip())\n",
    "        sorter.append(author_id)\n",
    "        \n",
    "        if author_id not in existing_personalities:\n",
    "            \n",
    "            print(f'Adding {author_id}')\n",
    "            \n",
    "            # Fetch personality page\n",
    "            url = re.search(r'<a href=\"(.*)\">', str(subject)).group(1)\n",
    "            html = requests.get(f'https://www.politifact.com{url}')\n",
    "            soup = BeautifulSoup(html.text, 'html.parser')\n",
    "            \n",
    "            error = soup.find('div', {'class': 'pfhead'})\n",
    "            \n",
    "            if error is None:\n",
    "                # Fetch and clean description and link\n",
    "                description_raw = soup.find('div', {'class': 'm-pageheader__body'}).text\n",
    "                link_raw = soup.find('footer', {'class': 'm-pageheader__footer'})\n",
    "                description = description_raw.strip()\n",
    "                link = re.search(r' href=\"(.*?)\"', str(link_raw)).group(1)\n",
    "            else:\n",
    "                description = \"\"\n",
    "                link = \"\"\n",
    "\n",
    "            # Create row\n",
    "            row = pd.DataFrame({\n",
    "                'author_id': [author_id],\n",
    "                'author_name': [author_name], \n",
    "                'affiliation': [affiliation], \n",
    "                'description': [description], \n",
    "                'link': [link],\n",
    "            })\n",
    "\n",
    "            # Append row to dataframe\n",
    "            df_raw = df_raw.append(row, ignore_index=True)\n",
    "\n",
    "            # Sleep for a few seconds, be nice to web servers :)\n",
    "            pause = random.randint(2, 4)\n",
    "            time.sleep(pause)\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Sort data the way it is presented originally\n",
    "    sorterIndex = dict(zip(sorter, range(len(sorter))))\n",
    "    df_raw['author_rank'] = df_raw['author_id'].map(sorterIndex)\n",
    "    df_raw.sort_values(by='author_rank', inplace=True)\n",
    "    df_raw.drop('author_rank', 1, inplace=True)\n",
    "    \n",
    "    print('Done fetching personalities!')\n",
    "    return(df_raw.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fetching personalities!\n"
     ]
    }
   ],
   "source": [
    "# If running for first time, simply remove argument df\n",
    "#personalities_df = FetchPersonalities()\n",
    "personalities_df = FetchPersonalities(df=pd.read_csv(metadata_path + 'personalities.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportDataFrame(df, filename):\n",
    "    \"\"\"Helper function to export dataframes\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): dataframe to export\n",
    "        filename (str): name of file to export\n",
    "    Returns:\n",
    "        None.\n",
    "        \n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export quotes dataframes\n",
    "#exportDataFrame(true_df, data_path + 'true.csv')\n",
    "exportDataFrame(mostly_true_df, data_path + 'mostly-true.csv')\n",
    "exportDataFrame(half_true_df, data_path + 'half-true.csv')\n",
    "exportDataFrame(barely_true_df, data_path + 'barely-true.csv')\n",
    "exportDataFrame(false_df, data_path + 'false.csv')\n",
    "exportDataFrame(pants_fire_df, data_path + 'pants-fire.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export personalities dataframes\n",
    "exportDataFrame(personalities_df, metadata_path + 'personalities.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
