{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import preprocessing\n",
    "import gender_guesser.detector as gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "Load all 6 datasets (`true`, `mostly-true`, `half-true`, `barely-true`, `false`, `pants-fire`) and combine them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18052 rows and 8 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>quote</th>\n",
       "      <th>context</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>categories</th>\n",
       "      <th>staff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>barely-true</td>\n",
       "      <td>“Pennsylvania just banned alcohol sales.”</td>\n",
       "      <td>a Facebook post</td>\n",
       "      <td>viral-image</td>\n",
       "      <td>Viral image</td>\n",
       "      <td>November 24, 2020</td>\n",
       "      <td>Facebook Fact-checks, Coronavirus</td>\n",
       "      <td>Ciara O'Rourke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>barely-true</td>\n",
       "      <td>“666,000 teachers have been laid off already s...</td>\n",
       "      <td>a virtual roundtable</td>\n",
       "      <td>joe-biden</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>November 18, 2020</td>\n",
       "      <td>Education, Coronavirus</td>\n",
       "      <td>Bill McCarthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>barely-true</td>\n",
       "      <td>“David Perdue says he'll do everything in his ...</td>\n",
       "      <td>an ad</td>\n",
       "      <td>jon-ossoff</td>\n",
       "      <td>Jon Ossoff</td>\n",
       "      <td>November 17, 2020</td>\n",
       "      <td>Georgia, Negative Campaigning</td>\n",
       "      <td>Tom Kertscher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>barely-true</td>\n",
       "      <td>Says “47 additional counties used the same sof...</td>\n",
       "      <td>a Facebook post</td>\n",
       "      <td>ted-nugent</td>\n",
       "      <td>Ted Nugent</td>\n",
       "      <td>November 17, 2020</td>\n",
       "      <td>Elections, Facebook Fact-checks</td>\n",
       "      <td>Samantha Putterman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>barely-true</td>\n",
       "      <td>\"Voter FRAUD exposed in Georgia. Over 2600 vot...</td>\n",
       "      <td>in a Live video</td>\n",
       "      <td>facebook-posts</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>November 16, 2020</td>\n",
       "      <td>Georgia, Elections, Facebook Fact-checks</td>\n",
       "      <td>Daniel Funke</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                              quote  \\\n",
       "0  barely-true          “Pennsylvania just banned alcohol sales.”   \n",
       "1  barely-true  “666,000 teachers have been laid off already s...   \n",
       "2  barely-true  “David Perdue says he'll do everything in his ...   \n",
       "3  barely-true  Says “47 additional counties used the same sof...   \n",
       "4  barely-true  \"Voter FRAUD exposed in Georgia. Over 2600 vot...   \n",
       "\n",
       "                context       author_id     author_name               date  \\\n",
       "0       a Facebook post     viral-image     Viral image  November 24, 2020   \n",
       "1  a virtual roundtable       joe-biden       Joe Biden  November 18, 2020   \n",
       "2                 an ad      jon-ossoff      Jon Ossoff  November 17, 2020   \n",
       "3       a Facebook post      ted-nugent      Ted Nugent  November 17, 2020   \n",
       "4       in a Live video  facebook-posts  Facebook posts  November 16, 2020   \n",
       "\n",
       "                                 categories               staff  \n",
       "0         Facebook Fact-checks, Coronavirus      Ciara O'Rourke  \n",
       "1                    Education, Coronavirus       Bill McCarthy  \n",
       "2             Georgia, Negative Campaigning       Tom Kertscher  \n",
       "3           Elections, Facebook Fact-checks  Samantha Putterman  \n",
       "4  Georgia, Elections, Facebook Fact-checks        Daniel Funke  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in and append dataframes\n",
    "data_path = 'data/'\n",
    "df_quotes = pd.DataFrame()\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename in ['true.csv', 'mostly-true.csv', 'half-true.csv', 'barely-true.csv', 'false.csv', 'pants-fire.csv']:\n",
    "        df_quotes = df_quotes.append(pd.read_csv(data_path + filename, dtype={'label':str}, na_values='unspecified'), ignore_index=True)\n",
    "\n",
    "print(f'There are {df_quotes.shape[0]} rows and {df_quotes.shape[1]} columns.')\n",
    "df_quotes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Variables from Quotes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplicate Entries\n",
    "Ensure that there are no duplicated entries. For some reason, there are duplicated entries for certain quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 18052 rows\n",
      "After: 18046 rows\n"
     ]
    }
   ],
   "source": [
    "# These are quote entries that are identical across the board... we remove them\n",
    "dup_quotes = df_quotes.loc[df_quotes.duplicated()]\n",
    "\n",
    "print(f'Before: {df_quotes.shape[0]} rows')\n",
    "df_quotes = df_quotes.drop_duplicates()\n",
    "print(f'After: {df_quotes.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Binary True False Target Variable\n",
    "Although it is great to have a more detailed classification of each quote by having as many as 6 different levels of *realness*, it would nevertheless be interesting to simply have a binary output of whether a quote is **true or false**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotes['label_binary'] = np.where(df_quotes['label'].isin(['true', 'mostly-true']), 'true', 'false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Target Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "df_quotes['label'] = encoder.fit_transform(df_quotes['label'])\n",
    "df_quotes['label_binary'] = encoder.fit_transform(df_quotes['label_binary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Normalize Context\n",
    "There are more than **5000** unique strings in the `context` variable, which makes it difficult to use as features. As such, we attempt to clean up the entries by reducing the number of possible categories. For instance, contexts that contain the word *email* or *e-mail* would be classified as simply *email*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5984 unique contexts. Cleaning...\n",
      "284 unique contexts. Cleaning...\n",
      "279 unique contexts. Cleaning...\n",
      "Number of unique contexts after cleaning: 279.\n"
     ]
    }
   ],
   "source": [
    "# Create new variable context_clean\n",
    "df_quotes['context_clean'] = df_quotes['context']\n",
    "\n",
    "# While there are still sub-strings that can be cleaned, continue\n",
    "n_rows = 0\n",
    "while len(df_quotes['context_clean'].value_counts()) != n_rows:\n",
    "    \n",
    "    n_rows = len(df_quotes['context_clean'].value_counts())\n",
    "    \n",
    "    # Set to lower\n",
    "    df_quotes['context_clean'] = df_quotes['context_clean'].str.lower()\n",
    "    # Remove leading and trailing . and spaces\n",
    "    df_quotes['context_clean'] = df_quotes['context_clean'].str.strip('\\. ')\n",
    "    # Remove leading words (in, on, his, her, the, our, their)\n",
    "    df_quotes['context_clean'] = df_quotes['context_clean'].str.replace('^(in |on |his |her |the |our |their )', '', regex=True)\n",
    "    # Remove leading words (a, an) Note: Done after previous line because of cases like \"in a\"\n",
    "    df_quotes['context_clean'] = df_quotes['context_clean'].str.replace('^(a |an )', '', regex=True)\n",
    "    # Triage of contexts based on words contained (ie. the television episode about climate change ---> TV)\n",
    "    context_dict = {'commercial$':'ad', 'advertisement':'ad', '.*e-mail.*':'email', '.*(^| )email.*':'email', '.*op-ed.*':'op-ed', '.*(^| )oped.*':'op-ed', '.*facebook.*':'facebook', 'television':'tv', '.*(^| )tv( |$).*':'tv', '.*(^| )video( |$).*':'video', '.*image.*':'image', '.*twitter.*':'tweet', '.*tweet.*':'tweet', '.*(^| )reddit.*':'reddit', '.*meet the press.*':'mtp', '.*(^| )press( |$).*':'press', '.*blog.*':'blog', '.*radio.*':'radio', '.*instagram.*':'instagram', '.*nbc.*':'msnbc', '.*fox news sunday.*':'fns', '.*cnn.*':'cnn', '.*abc.*':'abc', '.*this week.*':'abc', '.*fox.*':'fox news', '.*cbs.*':'cbs', '.*face the nation.*':'cbs', '.*cpac.*':'cpac', '.*the daily show.*':'the daily show', '.*hbo.*':'hbo', '.*senate floor speech.*':'senate floor speech', '.*newspaper.*':'newspaper', '.*senate floor.*':'senate floor', '.*(^| )letter.*':'letter', '.*newsletter.*':'newsletter', '.*(^| )debate.*':'debate', '.*(^| )flier.*':'flier', '.*flyer.*':'flier', '.*hearing.*':'hearing', '.*mailer.*':'mailer', '.*social media.*':'social media', '.*petition.*':'petition', '.*(^| )book.*':'book', '.*(^| )ad.*':'ad', '.*(^| )speech.*':'speech', '.*interview.*':'interview', '.*(^| )web.*':'web', '.*article.*':'article', '.*(^| )comment.*':'comment', '.*(^| )story.*':'story', '.*(^| )remarks.*':'remarks', '.*conversation.*':'conversation', '.*medium.*':'medium', '.*statement.*':'statement', '.*editorial.*':'editorial', '.*meeting.*':'meeting', '.*campaign.*':'campaign', '.*(^| )news( |$).*':'news', '.*column.*':'column', '.*reporters.*':'reporters', '.*online.*':'online', '.*survey.*':'survey', '.*study.*':'study', '.*briefing.*':'briefing', '.*questionnaire.*':'survey', '.*internet.*':'online', '.*rally.*':'rally', '.*town hall.*':'town hall', '.*brochure.*':'brochure', '.*monologue.*':'speech', '.*senate.*':'senate', '.*presentation.*':'presentation', '.*forum.*':'forum', '.*opinion.*':'opinion', '.*call.*':'call', '.*congress.*':'congress', '.*youtube.*':'video', '.*telev.*':'tv', '.*(^| )show.*':'show', '.*testimony.*':'testimony', '.*discussion.*':'discussion', '.*(^| )text.*':'text', '.*(^| )event.*':'event', '.*(^| )episode.*':'episode', '.*(^| )resolution.*':'resolution', '.*(^| )pamphlet.*':'brochure', '.*(^| )question.*':'question', '.*(^| )document.*':'document', '.*(^| )report.*':'report', '.*(^| )appearance.*':'appearance', '.*(^| )response.*':'response', '.*(^| )protest.*':'protest', '.*(^| )broadcast.*':'broadcast', '.*(^| )proposal.*':'proposal', '.*(^| )essay.*':'essay', '.*(^| )sign.*':'sign', '.*(^| )summit.*':'summit', '.*(^| )conference.*':'conference', '.*(^| )trial.*':'trial', '.*(^| )session.*':'session', '.*(^| )snapchat.*':'snapchat', '.*(^| )paper.*':'paper', '.*(^| )graphic.*':'graphic', '.*(^| )magazine.*':'magazine', '.*(^| )convention.*':'convention', '.*(^| )memo.*':'memo', '.*(^| )roundtable.*':'roundtable', '.*(^| )chat.*':'conversation', '.*(^| )rebuttal.*':'rebuttal', '.*(^| )message.*':'message', '.*(^| )guide.*':'guide', '.*(^| )meme.*':'meme', '.*(^| )plan.*':'plan', '.*(^| )fair.*':'conference', '.*(^| )sketch.*':'sketch', '.*(^| )screenshot.*':'screenshot', '.*(^| )attachment.*':'attachment', '.*(^| )infographic.*':'graphic', '.*(^| )widget.*':'widget', '.*(^| )bill.*':'bill', '.*(^| )townhall.*':'town hall', '.*(^| )comic.*':'comic', '.*(^| )poll.*':'poll', '.*(^| )talk.*':'presentation', '.*(^| )ordinance.*':'ordinance', '.*(^| )decision.*':'decision', '.*(^| )meetup.*':'meetup', '.*(^| )direct-mail.*':'mail', '.*(^| )teleconference.*':'conference', '.*(^| )gathering.*':'gathering', '.*(^| )mailing.*':'mail', '.*(^| )fundraiser.*':'event', '.*(^| )post.*':'post', '.*(^| )show.*':'show', '.*(^| )segment.*':'segment', '.*(^| )cartoon.*':'comic', '.*(^| )brief.*':'briefing', '.*(^| )announcement.*':'announcement', '.*(^| )luncheon.*':'gathering', '.*(^| )panel.*':'panel', '.*(^| )emai.*':'email', '.*(^| )speaking.*':'presentation', '.*(^| )argument.*':'argument', '.*(^| )visit.*':'visit', '.*(^| )media.*':'media', '.*(^| )site.*':'web', '.*(^| )remaks.*':'remarks', '.*(^| )announcing.*':'announcement', '.*(^| )mail.*':'mail', '.*(^| )stories.*':'story', '.*(^| )photo.*':'image', '.*(^| )leaflet.*':'brochure', '.*(^| )concert.*':'concert', '.*(^| )ceremony.*':'ceremony', '.*(^| )reception.*':'reception', '.*quote.*':'quote', '.*(^| )encyclopedia.*':'encyclopedia', '.*(^| )journal.*':'journal', '.*(^| )orientation.*':'orientation', '.*(^| )fundraising.*':'event', '.*(^| )pitch.*':'pitch', '.*(^| )lecture.*':'presentation', '.*(^| )confrontation.*':'argument', '.*(^| )ruling.*':'ruling', '.*(^| )retreat.*':'gathering', '.*(^| )symposium.*':'conference', '.*(^| )segment.*':'segment'}\n",
    "    df_quotes['context_clean'] = df_quotes['context_clean'].replace(context_dict, regex=True)\n",
    "    \n",
    "    print(f'{n_rows} unique contexts. Cleaning...')\n",
    "\n",
    "print(f'Number of unique contexts after cleaning: {n_rows}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after cleaning and grouping many of the contexts, there are still a lot of categories with very few occurrences. In an attempt to further simplify the classification, any context class with fewer than 10 occurrences will be group together as *other*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of unique contexts: 76\n"
     ]
    }
   ],
   "source": [
    "# Set all contexts with less than 10 occurrences to the category 'other'\n",
    "df_quotes.loc[df_quotes.groupby('context_clean')['context_clean'].transform('count').lt(10), 'context_clean'] = 'other'\n",
    "print(f\"Final number of unique contexts: {len(df_quotes['context_clean'].value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data verficiation checks\n",
    "#df_quotes.loc[df_quotes['context_clean'].str.contains('other')]\n",
    "#df_quotes['context_clean'].value_counts().loc[df_quotes['context_clean'].value_counts().index.str.contains('tiktok')][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Normalize Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "markel-hutchins          1\n",
       "ideas-illinois           1\n",
       "politico-news            1\n",
       "thomas-norment-jr        1\n",
       "doug-muder               1\n",
       "                        ..\n",
       "marco-rubios-heckler     1\n",
       "martha-robertson         1\n",
       "jack-seiler              1\n",
       "pro-conservative-news    1\n",
       "southwest-farm-press     1\n",
       "Name: author_id, Length: 2616, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data verification checks\n",
    "df_quotes['author_id'].value_counts()[:50]\n",
    "df_quotes['author_id'].value_counts().loc[df_quotes['author_id'].value_counts() < 2]\n",
    "\n",
    "# In the event we want to replace authors with fewer than X occurrences\n",
    "#df_quotes['author_clean'] = df_quotes['author_id']\n",
    "#df_quotes.loc[df_quotes.groupby('author_clean')['author_clean'].transform('count').lt(2), 'author_clean'] = 'other'\n",
    "#df_quotes['author_clean'].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata\n",
    "Load additional information on authors and merge to main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13th-district-gop-slate</td>\n",
       "      <td>13th District GOP slate</td>\n",
       "      <td>Republican</td>\n",
       "      <td>The 13th District GOP slate includes state Sen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18-percent-american-public</td>\n",
       "      <td>18% of the American public</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60-plus-association</td>\n",
       "      <td>60 Plus Association</td>\n",
       "      <td>None</td>\n",
       "      <td>The 60 Plus Association is a conservative advo...</td>\n",
       "      <td>http://www.60plus.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AARP</td>\n",
       "      <td>AARP</td>\n",
       "      <td>None</td>\n",
       "      <td>AARP is a nonprofit, nonpartisan organization ...</td>\n",
       "      <td>http://www.aarp.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>greg-abbott</td>\n",
       "      <td>Greg Abbott</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Greg Abbott won election as governor of Texas ...</td>\n",
       "      <td>http://gregabbott.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    author_id                 author_name affiliation  \\\n",
       "0     13th-district-gop-slate     13th District GOP slate  Republican   \n",
       "1  18-percent-american-public  18% of the American public        None   \n",
       "2         60-plus-association         60 Plus Association        None   \n",
       "3                        AARP                        AARP        None   \n",
       "4                 greg-abbott                 Greg Abbott  Republican   \n",
       "\n",
       "                                         description                    link  \n",
       "0  The 13th District GOP slate includes state Sen...                     NaN  \n",
       "1                                                NaN                     NaN  \n",
       "2  The 60 Plus Association is a conservative advo...  http://www.60plus.org/  \n",
       "3  AARP is a nonprofit, nonpartisan organization ...    http://www.aarp.org/  \n",
       "4  Greg Abbott won election as governor of Texas ...  http://gregabbott.com/  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in meta data\n",
    "metadata_path = 'metadata/'\n",
    "df_personalities = pd.read_csv(metadata_path + 'personalities.csv')\n",
    "df_personalities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplicate Entries\n",
    "Ensure that there are no duplicated entries. For some reason, there are duplicated entries for certain personalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 4643 rows\n",
      "After: 4610 rows\n"
     ]
    }
   ],
   "source": [
    "dup_personalities = df_personalities['author_id'][df_personalities['author_id'].duplicated()].unique()\n",
    "df_dup = df_personalities.loc[df_personalities['author_id'].isin(dup_personalities)]\n",
    "exceptions_to_drop = ['Billboard at Spaghetti Junction'] # Maps to another page...\n",
    "\n",
    "print(f'Before: {df_personalities.shape[0]} rows')\n",
    "df_personalities = df_personalities.drop_duplicates()\n",
    "df_personalities = df_personalities.loc[~df_personalities['author_name'].isin(exceptions_to_drop)]\n",
    "print(f'After: {df_personalities.shape[0]} rows')\n",
    "\n",
    "# Check to see if there are still duplicated author_id after\n",
    "if any(df_personalities.loc[df_personalities['author_id'].isin(dup_personalities)]['author_id'].duplicated()):\n",
    "    print('Warning! There are still duplicated author_id record(s).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive Author's Gender\n",
    "Using `gender-guesser`, we attempt to derive additional information of the author's gender based on their **first** names. Each author is classied into either `male`, `female`, `mostly male` and `mostly female`, whereas entries that are not names (ie. organizations, entities, etc.) are assigned `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male             2293\n",
       "unknown          1312\n",
       "female            706\n",
       "mostly_male       174\n",
       "mostly_female      96\n",
       "andy               29\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Derive gender from first name whenever possible\n",
    "gd = gender.Detector()\n",
    "unk_word_list = ['The', 'Young', 'My', 'In', 'Ban', 'Free'] # List of first words known to be misclassified\n",
    "unk_list = ['Al Jazeera America', 'Austin Board of Realtors PAC', 'Austin Fund for Quality Healthcare', 'Austin Independent School District', 'Austin Water Utility', 'Austin for a Better Future', 'Brady Campaign to Prevent Gun Violence', 'Christian Broadcasting Network', 'Clayton County Government', 'Clayton County Schools', 'Dane County Republican Party', 'Dustin Inman Society', 'Elle', 'Fair Districts Florida', 'Forbes blog', 'Georgia Association of Homes and Services for Children', 'Georgia Association of Latino Elected Officials', 'Georgia Craft Brewers Guild', 'Georgia Democrats', 'Georgia Department of Economic Development', 'Georgia Department of Public Health', 'Georgia Department of Transportation', 'Georgia Family Council', 'Georgia Farm Bureau', 'Georgia Green Party', 'Georgia Gun Owners', 'Georgia House Democratic Caucus on behalf of Elena Parent', 'Georgia Lottery', 'Georgia Restaurant Association', 'Georgia State Road and Tollway Authority', 'Georgia Voice', 'Georgia politicians', 'Georgia state senators', 'Gun Free UT Gun Free UT', 'Gun Owners of America', 'Hal Turner Radio Show', 'Save America\\'s Postal Service', 'Save Flexible Spending Plans', 'Save My Care', 'Save Our City, Milwaukeeans Can\\'t Wait', 'Save Our Springs Alliance', 'Sierra Club', 'Travis County Republican Party', 'Urban Intellectuals', 'Virginia Center for Public Safety', 'Virginia Education Association', 'Virginia First Foundation', 'Virginia House Democratic Caucus' ,'Virginia Interfaith Center for Public Policy', 'Virginia Lottery', 'Virginia Senate Democratic Caucus', 'Virginia Senate Democrats', 'Virginia Society for Human Life', 'Virginia Tea Party Patriots']\n",
    "df_personalities['gender'] = df_personalities['author_name'].apply(lambda x: gd.get_gender(x.split()[0]))\n",
    "df_personalities['gender'] = np.where(df_personalities['author_name'].str.split().str[0].isin(unk_word_list), 'unknown', df_personalities['gender'])\n",
    "df_personalities['gender'] = np.where(df_personalities['author_name'].isin(unk_list), 'unknown', df_personalities['gender'])\n",
    "df_personalities['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Misclassified Genders\n",
    "We leverage the description of each author to correct some of the misclassified genders by checking to see if there are contradictions between the assigned gender and the description. For instance, if an author is assigned the gender `male`, but the description only contains `she` and `her`, there are high chances that it is a misclassification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male             2387\n",
       "unknown          1312\n",
       "female            750\n",
       "mostly_male        92\n",
       "mostly_female      52\n",
       "andy               17\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of known males and females (hardcoded due to exceptions in following rules)\n",
    "male_list = ['Mike Martinez', 'Matt Tighe']\n",
    "female_list = ['Marco Rubio\\'s heckler', 'Sharron Angle', 'Ann Marie Buerkle', 'Kaya Jones', 'Jane O’Meara Sanders', 'Tiffany Trump', 'Lauren Kane', 'María Teresa Kumar', 'Rick Scott\\'s Starbucks heckler']\n",
    "\n",
    "tmp = df_personalities.copy()\n",
    "\n",
    "# Boolean variables to indicate presence of he/his or she/her in description\n",
    "tmp['male_check'] = tmp['description'].apply(lambda x: np.nan if pd.isnull(x) else any(word in x.lower().split() for word in ['he', 'his']))\n",
    "tmp['female_check'] = tmp['description'].apply(lambda x: np.nan if pd.isnull(x) else any(word in x.lower().split() for word in ['she', 'her']))\n",
    "\n",
    "# Correct mis-classified records by generating lists of authors who should actually be male or female\n",
    "# Logic: ie. if classified as male, but description only contains she/her, high probability is female\n",
    "m2f = [x for x in tmp.loc[(tmp['gender'] == 'male') & (tmp['male_check'] == False) & (tmp['female_check'] == True), 'author_name'] if x not in male_list]\n",
    "f2m = [x for x in tmp.loc[(tmp['gender'] == 'female') & (tmp['male_check'] == True) & (tmp['female_check'] == False), 'author_name'] if x not in female_list]\n",
    "mm2f = [x for x in tmp.loc[(tmp['gender'] == 'mostly_male') & (tmp['male_check'] == False) & (tmp['female_check'] == True), 'author_name'] if x not in male_list]\n",
    "mm2m = [x for x in tmp.loc[(tmp['gender'] == 'mostly_male') & (tmp['male_check'] == True) & (tmp['female_check'] == False), 'author_name'] if x not in female_list]\n",
    "mf2f = [x for x in tmp.loc[(tmp['gender'] == 'mostly_female') & (tmp['male_check'] == False) & (tmp['female_check'] == True), 'author_name'] if x not in male_list]\n",
    "mf2m = [x for x in tmp.loc[(tmp['gender'] == 'mostly_female') & (tmp['male_check'] == True) & (tmp['female_check'] == False), 'author_name'] if x not in female_list]\n",
    "a2m = [x for x in tmp.loc[(tmp['gender'] == 'andy') & (tmp['male_check'] == True) & (tmp['female_check'] == False), 'author_name'] if x not in female_list]\n",
    "a2f = [x for x in tmp.loc[(tmp['gender'] == 'andy') & (tmp['male_check'] == False) & (tmp['female_check'] == True), 'author_name'] if x not in male_list]\n",
    "\n",
    "# Correct mis-classified genders \n",
    "df_personalities['gender'] = np.where(df_personalities['author_name'].isin(male_list + f2m + mm2m + mf2m + a2m), 'male', \n",
    "                                      np.where(df_personalities['author_name'].isin(female_list + m2f + mm2f + mf2f + a2f), 'female', \n",
    "                                          df_personalities['gender']))\n",
    "\n",
    "df_personalities['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge meta data\n",
    "df_full = df_quotes.merge(df_personalities, on=['author_id', 'author_name'], how='left')\n",
    "if df_quotes.shape[0] != df_full.shape[0]:\n",
    "    print('Warning! There are more rows than before!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>quote</th>\n",
       "      <th>context</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>categories</th>\n",
       "      <th>staff</th>\n",
       "      <th>label_binary</th>\n",
       "      <th>context_clean</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>“Pennsylvania just banned alcohol sales.”</td>\n",
       "      <td>a Facebook post</td>\n",
       "      <td>viral-image</td>\n",
       "      <td>Viral image</td>\n",
       "      <td>November 24, 2020</td>\n",
       "      <td>Facebook Fact-checks, Coronavirus</td>\n",
       "      <td>Ciara O'Rourke</td>\n",
       "      <td>0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>None</td>\n",
       "      <td>Graphics, pictures and charts shared on social...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>“666,000 teachers have been laid off already s...</td>\n",
       "      <td>a virtual roundtable</td>\n",
       "      <td>joe-biden</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>November 18, 2020</td>\n",
       "      <td>Education, Coronavirus</td>\n",
       "      <td>Bill McCarthy</td>\n",
       "      <td>0</td>\n",
       "      <td>roundtable</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Joe Biden is President-elect of the United Sta...</td>\n",
       "      <td>https://www.joebiden.com/</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>“David Perdue says he'll do everything in his ...</td>\n",
       "      <td>an ad</td>\n",
       "      <td>jon-ossoff</td>\n",
       "      <td>Jon Ossoff</td>\n",
       "      <td>November 17, 2020</td>\n",
       "      <td>Georgia, Negative Campaigning</td>\n",
       "      <td>Tom Kertscher</td>\n",
       "      <td>0</td>\n",
       "      <td>ad</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Jon Ossoff is a Democrat running to succeed fo...</td>\n",
       "      <td>https://electjon.com/</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Says “47 additional counties used the same sof...</td>\n",
       "      <td>a Facebook post</td>\n",
       "      <td>ted-nugent</td>\n",
       "      <td>Ted Nugent</td>\n",
       "      <td>November 17, 2020</td>\n",
       "      <td>Elections, Facebook Fact-checks</td>\n",
       "      <td>Samantha Putterman</td>\n",
       "      <td>0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Ted Nugent, who lives near Waco, performed aft...</td>\n",
       "      <td>http://www.tednugent.com/</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Voter FRAUD exposed in Georgia. Over 2600 vot...</td>\n",
       "      <td>in a Live video</td>\n",
       "      <td>facebook-posts</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>November 16, 2020</td>\n",
       "      <td>Georgia, Elections, Facebook Fact-checks</td>\n",
       "      <td>Daniel Funke</td>\n",
       "      <td>0</td>\n",
       "      <td>video</td>\n",
       "      <td>None</td>\n",
       "      <td>Posters on Facebook and other social media net...</td>\n",
       "      <td>https://www.facebook.com/</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              quote  \\\n",
       "0      0          “Pennsylvania just banned alcohol sales.”   \n",
       "1      0  “666,000 teachers have been laid off already s...   \n",
       "2      0  “David Perdue says he'll do everything in his ...   \n",
       "3      0  Says “47 additional counties used the same sof...   \n",
       "4      0  \"Voter FRAUD exposed in Georgia. Over 2600 vot...   \n",
       "\n",
       "                context       author_id     author_name               date  \\\n",
       "0       a Facebook post     viral-image     Viral image  November 24, 2020   \n",
       "1  a virtual roundtable       joe-biden       Joe Biden  November 18, 2020   \n",
       "2                 an ad      jon-ossoff      Jon Ossoff  November 17, 2020   \n",
       "3       a Facebook post      ted-nugent      Ted Nugent  November 17, 2020   \n",
       "4       in a Live video  facebook-posts  Facebook posts  November 16, 2020   \n",
       "\n",
       "                                 categories               staff  label_binary  \\\n",
       "0         Facebook Fact-checks, Coronavirus      Ciara O'Rourke             0   \n",
       "1                    Education, Coronavirus       Bill McCarthy             0   \n",
       "2             Georgia, Negative Campaigning       Tom Kertscher             0   \n",
       "3           Elections, Facebook Fact-checks  Samantha Putterman             0   \n",
       "4  Georgia, Elections, Facebook Fact-checks        Daniel Funke             0   \n",
       "\n",
       "  context_clean affiliation  \\\n",
       "0      facebook        None   \n",
       "1    roundtable    Democrat   \n",
       "2            ad    Democrat   \n",
       "3      facebook  Republican   \n",
       "4         video        None   \n",
       "\n",
       "                                         description  \\\n",
       "0  Graphics, pictures and charts shared on social...   \n",
       "1  Joe Biden is President-elect of the United Sta...   \n",
       "2  Jon Ossoff is a Democrat running to succeed fo...   \n",
       "3  Ted Nugent, who lives near Waco, performed aft...   \n",
       "4  Posters on Facebook and other social media net...   \n",
       "\n",
       "                        link   gender  \n",
       "0                        NaN  unknown  \n",
       "1  https://www.joebiden.com/     male  \n",
       "2      https://electjon.com/     male  \n",
       "3  http://www.tednugent.com/     male  \n",
       "4  https://www.facebook.com/  unknown  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date related features\n",
    "# TODO: Fix unspecified dates...\n",
    "df_full.loc[df_full['date'] == 'unspecified', 'date'] = np.nan\n",
    "df_full['date_formatted'] = pd.to_datetime(df_full['date'], format='%B %d, %Y')\n",
    "df_full['date_year'] = df_full['date_formatted'].dt.year.astype('Int64')\n",
    "df_full['date_month'] = df_full['date_formatted'].dt.month.astype('Int64')\n",
    "df_full['date_day'] = df_full['date_formatted'].dt.day.astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quote related features\n",
    "df_full['num_words'] = df_full['quote'].str.split().str.len()\n",
    "df_full['num_chars'] = df_full['quote'].str.len()\n",
    "df_full['avg_word_len'] = df_full['quote'].apply(lambda x: round((sum(len(word) for word in x.split()) / len(x.split())), 1))\n",
    "df_full['num_stopwords'] = df_full['quote'].apply(lambda x: len([w for w in x.split() if w.lower() in stopwords.words('english')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical variables\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "features = ['num_words', 'num_chars', 'avg_word_len', 'num_stopwords']\n",
    "for feat in features:\n",
    "    df_full[[feat]] = scaler.fit_transform(df_full[[feat]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot encode author id\n",
    "df_full = df_full.merge(pd.get_dummies(df_full['author_id'].str.lower(), prefix='author'),\n",
    "                         left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot encode context\n",
    "df_full = df_full.merge(pd.get_dummies(df_full['context_clean'].str.lower(), prefix='context'),\n",
    "                         left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up affiliation variables\n",
    "# Any category with fewer than 10 occurrences are set to \"Other\"\n",
    "df_full.loc[df_full.groupby('affiliation')['affiliation'].transform('count').lt(10), 'affiliation'] = 'Other'\n",
    "# One-Hot encode affiliation\n",
    "df_full = df_full.merge(pd.get_dummies(df_full['affiliation'].str.lower(), prefix='affiliation'),\n",
    "                         left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot encode gender\n",
    "df_full = df_full.merge(pd.get_dummies(df_full['gender'].str.lower(), prefix='gender'),\n",
    "                         left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_words = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return ''.join(new_words)\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()   \n",
    "    new_words = []\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        new_words.append(str(lemmatizer.lemmatize(word)))\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def preprocess(df, t):\n",
    "    df[t] = df[t].apply(lambda x : x.lower()) #Lower case everything\n",
    "    df[t] = df[t].apply(lambda x : re.sub(r'[^\\w\\s]', '', x)) #Remove punctuation \n",
    "    df[t] = df[t].apply(lambda x : remove_non_ascii(x))  #Removing Non ASCII Words\n",
    "    df[t] = df[t].apply(lambda x : remove_stopwords(x))#Removing Stopwords\n",
    "    df[t] = df[t].apply(lambda x : lemmatize_words(x))#Lemmatize words \n",
    "    return df[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['quote'] = preprocess(df_full,'quote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>quote</th>\n",
       "      <th>context</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>date</th>\n",
       "      <th>categories</th>\n",
       "      <th>staff</th>\n",
       "      <th>label_binary</th>\n",
       "      <th>context_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>affiliation_other</th>\n",
       "      <th>affiliation_republican</th>\n",
       "      <th>affiliation_state official</th>\n",
       "      <th>affiliation_talk show host</th>\n",
       "      <th>gender_andy</th>\n",
       "      <th>gender_female</th>\n",
       "      <th>gender_male</th>\n",
       "      <th>gender_mostly_female</th>\n",
       "      <th>gender_mostly_male</th>\n",
       "      <th>gender_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>pennsylvania banned alcohol sale</td>\n",
       "      <td>a Facebook post</td>\n",
       "      <td>viral-image</td>\n",
       "      <td>Viral image</td>\n",
       "      <td>November 24, 2020</td>\n",
       "      <td>Facebook Fact-checks, Coronavirus</td>\n",
       "      <td>Ciara O'Rourke</td>\n",
       "      <td>0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>666000 teacher laid already since march</td>\n",
       "      <td>a virtual roundtable</td>\n",
       "      <td>joe-biden</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>November 18, 2020</td>\n",
       "      <td>Education, Coronavirus</td>\n",
       "      <td>Bill McCarthy</td>\n",
       "      <td>0</td>\n",
       "      <td>roundtable</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>david perdue say hell everything power make su...</td>\n",
       "      <td>an ad</td>\n",
       "      <td>jon-ossoff</td>\n",
       "      <td>Jon Ossoff</td>\n",
       "      <td>November 17, 2020</td>\n",
       "      <td>Georgia, Negative Campaigning</td>\n",
       "      <td>Tom Kertscher</td>\n",
       "      <td>0</td>\n",
       "      <td>ad</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>say 47 additional county used software caused ...</td>\n",
       "      <td>a Facebook post</td>\n",
       "      <td>ted-nugent</td>\n",
       "      <td>Ted Nugent</td>\n",
       "      <td>November 17, 2020</td>\n",
       "      <td>Elections, Facebook Fact-checks</td>\n",
       "      <td>Samantha Putterman</td>\n",
       "      <td>0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>voter fraud exposed georgia 2600 vote found</td>\n",
       "      <td>in a Live video</td>\n",
       "      <td>facebook-posts</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>November 16, 2020</td>\n",
       "      <td>Georgia, Elections, Facebook Fact-checks</td>\n",
       "      <td>Daniel Funke</td>\n",
       "      <td>0</td>\n",
       "      <td>video</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4362 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              quote  \\\n",
       "0      0                   pennsylvania banned alcohol sale   \n",
       "1      0            666000 teacher laid already since march   \n",
       "2      0  david perdue say hell everything power make su...   \n",
       "3      0  say 47 additional county used software caused ...   \n",
       "4      0        voter fraud exposed georgia 2600 vote found   \n",
       "\n",
       "                context       author_id     author_name               date  \\\n",
       "0       a Facebook post     viral-image     Viral image  November 24, 2020   \n",
       "1  a virtual roundtable       joe-biden       Joe Biden  November 18, 2020   \n",
       "2                 an ad      jon-ossoff      Jon Ossoff  November 17, 2020   \n",
       "3       a Facebook post      ted-nugent      Ted Nugent  November 17, 2020   \n",
       "4       in a Live video  facebook-posts  Facebook posts  November 16, 2020   \n",
       "\n",
       "                                 categories               staff  label_binary  \\\n",
       "0         Facebook Fact-checks, Coronavirus      Ciara O'Rourke             0   \n",
       "1                    Education, Coronavirus       Bill McCarthy             0   \n",
       "2             Georgia, Negative Campaigning       Tom Kertscher             0   \n",
       "3           Elections, Facebook Fact-checks  Samantha Putterman             0   \n",
       "4  Georgia, Elections, Facebook Fact-checks        Daniel Funke             0   \n",
       "\n",
       "  context_clean  ... affiliation_other affiliation_republican  \\\n",
       "0      facebook  ...                 0                      0   \n",
       "1    roundtable  ...                 0                      0   \n",
       "2            ad  ...                 0                      0   \n",
       "3      facebook  ...                 0                      1   \n",
       "4         video  ...                 0                      0   \n",
       "\n",
       "  affiliation_state official affiliation_talk show host gender_andy  \\\n",
       "0                          0                          0           0   \n",
       "1                          0                          0           0   \n",
       "2                          0                          0           0   \n",
       "3                          0                          0           0   \n",
       "4                          0                          0           0   \n",
       "\n",
       "   gender_female  gender_male  gender_mostly_female  gender_mostly_male  \\\n",
       "0              0            0                     0                   0   \n",
       "1              0            1                     0                   0   \n",
       "2              0            1                     0                   0   \n",
       "3              0            1                     0                   0   \n",
       "4              0            0                     0                   0   \n",
       "\n",
       "   gender_unknown  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               1  \n",
       "\n",
       "[5 rows x 4362 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discard Rows with Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 18046 rows.\n",
      "After: 17906 rows.\n"
     ]
    }
   ],
   "source": [
    "# Some rows don't have dates (bug in scraping?), so we will just discard them for now...\n",
    "print(f'Before: {df_full.shape[0]} rows.')\n",
    "df_full = df_full.loc[~df_full['date_formatted'].isnull()]\n",
    "print(f'After: {df_full.shape[0]} rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv('data/df_quotes_clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
